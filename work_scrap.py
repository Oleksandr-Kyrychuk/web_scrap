import requests
from bs4 import BeautifulSoup
import csv
import logging
import re
import time
import unicodedata
import random
import os
import gzip
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(filename="workua_scraper.log", level=logging.INFO)

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–µ–±–∞–≥—ñ–Ω–≥—É
DEBUG_MODE = False  # –£–≤—ñ–º–∫–Ω—É—Ç–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö HTML-—Ñ–∞–π–ª—ñ–≤


def create_vacancy_pattern(search_vacancy):
    """
    –°—Ç–≤–æ—Ä—é—î —Ä–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ –¥–ª—è –ø–æ—à—É–∫—É –≤–∞–∫–∞–Ω—Å—ñ–π –∑–∞ –≤–≤–µ–¥–µ–Ω–∏–º –∑–∞–ø–∏—Ç–æ–º.

    Args:
        search_vacancy (str): –ù–∞–∑–≤–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó –¥–ª—è –ø–æ—à—É–∫—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–í–æ–¥—ñ–π").

    Returns:
        str: –†–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –Ω–∞–∑–≤–∏ –≤–∞–∫–∞–Ω—Å—ñ—ó.
    """
    vacancy_base = search_vacancy.lower().strip()
    vacancy_base = re.escape(vacancy_base)
    vacancy_base = unicodedata.normalize("NFKD", vacancy_base).replace("—ñ", "i").replace("—ó", "i")
    vacancy_pattern = rf'\b(?:{vacancy_base}(?:[-\s][–∞-—è—ñ—ó—î“ë]+)*(?:[,.\s]*(?:–∫–∞—Ç\.?|–∫–∞—Ç–µ–≥–æ—Ä—ñ—ó?)\s*[A-Z–ê-–Ø–Ü–á–Ñ“ê]+(?:[,\s]*[A-Z–ê-–Ø–Ü–á–Ñ“ê]+)*)?(?:[,.\s\(][–∞-—è—ñ—ó—î“ë\s\(\)]+)?)\b'
    return vacancy_pattern


def convert_iso_to_text(iso_date):
    """
    –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î –¥–∞—Ç—É —É —Ñ–æ—Ä–º–∞—Ç—ñ ISO (YYYY-MM-DD HH:MM:SS) —É —Ç–µ–∫—Å—Ç–æ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç (DD –º—ñ—Å—è—Ü—å YYYY).

    Args:
        iso_date (str): –î–∞—Ç–∞ —É —Ñ–æ—Ä–º–∞—Ç—ñ ISO (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "2025-04-15 12:00:00").

    Returns:
        str: –î–∞—Ç–∞ —É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "15 –∫–≤—ñ—Ç–Ω—è 2025") –∞–±–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –¥–∞—Ç–∞, —è–∫—â–æ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π.
    """
    try:
        date_obj = datetime.strptime(iso_date, "%Y-%m-%d %H:%M:%S")
        months = ["—Å—ñ—á–Ω—è", "–ª—é—Ç–æ–≥–æ", "–±–µ—Ä–µ–∑–Ω—è", "–∫–≤—ñ—Ç–Ω—è", "—Ç—Ä–∞–≤–Ω—è", "—á–µ—Ä–≤–Ω—è",
                  "–ª–∏–ø–Ω—è", "—Å–µ—Ä–ø–Ω—è", "–≤–µ—Ä–µ—Å–Ω—è", "–∂–æ–≤—Ç–Ω—è", "–ª–∏—Å—Ç–æ–ø–∞–¥–∞", "–≥—Ä—É–¥–Ω—è"]
        return f"{date_obj.day} {months[date_obj.month - 1]} {date_obj.year}"
    except ValueError:
        return iso_date


# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Selenium –¥–ª—è Firefox
firefox_options = Options()
# firefox_options.add_argument("--headless")  # –í–∏–º–∫–Ω–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
firefox_options.add_argument("--no-sandbox")
firefox_options.add_argument("--disable-dev-shm-usage")

# –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
firefox_options.set_preference("general.useragent.override",
                               "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:129.0) Gecko/20100101 Firefox/129.0")

service = Service()
driver = webdriver.Firefox(service=service, options=firefox_options)

# –í–≤–µ–¥–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—É
search_vacancy = input("üîç –í–≤–µ–¥—ñ—Ç—å –Ω–∞–∑–≤—É –≤–∞–∫–∞–Ω—Å—ñ—ó: ").strip().lower()
search_city = input("üåÜ –í–≤–µ–¥—ñ—Ç—å –º—ñ—Å—Ç–æ: ").strip().lower()
page_input = input("üìÑ –í–≤–µ–¥—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–∞–±–æ '–≤—Å—ñ'): ").strip().lower()

# –í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤–≤–µ–¥–µ–Ω–Ω—è
if not search_vacancy or not search_city:
    print("‚ùå –ü–æ–º–∏–ª–∫–∞: –≤–∞–∫–∞–Ω—Å—ñ—è —Ç–∞ –º—ñ—Å—Ç–æ –Ω–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏!")
    driver.quit()
    exit(1)

# –§–æ—Ä–º—É—î–º–æ –±–∞–∑–æ–≤–∏–π URL
search_query = "-".join(search_vacancy.split())
search_city_query = "-".join(search_city.split())
base_url = f"https://www.work.ua/jobs-{search_city_query}-{search_query}/"

# –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫
if page_input == "–≤—Å—ñ":
    driver.get(base_url)
    # –Ø–≤–Ω–µ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "pagination"))
        )
    except Exception as e:
        logging.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é: {str(e)}")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    pagination = soup.find("ul", class_="pagination")
    max_pages = 1
    if pagination:
        try:
            page_links = pagination.find_all("li")
            for li in page_links[::-1]:
                a_tag = li.find("a")
                if a_tag and "page" in a_tag.get("href", ""):
                    max_pages = int(a_tag["href"].split("page=")[-1])
                    break
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {max_pages} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
        except (IndexError, ValueError):
            logging.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 1 —Å—Ç–æ—Ä—ñ–Ω–∫—É")
    else:
        logging.warning("–ü–∞–≥—ñ–Ω–∞—Ü—ñ—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞, –æ–±—Ä–æ–±–ª—è—î–º–æ –ª–∏—à–µ 1 —Å—Ç–æ—Ä—ñ–Ω–∫—É")
else:
    try:
        max_pages = int(page_input)
        if max_pages < 1:
            print("‚ùå –ü–æ–º–∏–ª–∫–∞: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –º–∞—î –±—É—Ç–∏ –±—ñ–ª—å—à–µ 0!")
            driver.quit()
            exit(1)
    except ValueError:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞: –≤–≤–µ–¥—ñ—Ç—å —á–∏—Å–ª–æ –∞–±–æ '–≤—Å—ñ'!")
        driver.quit()
        exit(1)

logging.info(f"–ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É: –≤–∞–∫–∞–Ω—Å—ñ—è={search_vacancy}, –º—ñ—Å—Ç–æ={search_city}, —Å—Ç–æ—Ä—ñ–Ω–æ–∫={max_pages}")

jobs = []

page = 1
while page <= max_pages:
    if page == 1:
        driver.get(base_url)
    else:
        # –Ü–º—ñ—Ç—É—î–º–æ –∫–ª—ñ–∫ –ø–æ –∫–Ω–æ–ø—Ü—ñ "–ù–∞—Å—Ç—É–ø–Ω–∞"
        try:
            next_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable(
                    (By.XPATH, "//a[contains(@class, 'link-icon') and .//span[text()='–ù–∞—Å—Ç—É–ø–Ω–∞']]"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
            current_url = driver.current_url
            next_button.click()
            WebDriverWait(driver, 10).until(
                lambda d: d.current_url != current_url
            )
        except Exception as e:
            logging.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É {page}: {str(e)}")
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–º–∏–ª–∫–∏
            with gzip.open(f"page_{page}_error.html.gz", "wt", encoding="utf-8") as f:
                f.write(driver.page_source)
            logging.info(f"HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page} –∑–±–µ—Ä–µ–∂–µ–Ω–æ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É –≤ page_{page}_error.html.gz")
            break

    # –Ø–≤–Ω–µ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ PJAX
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "pjax"))
        )
    except Exception as e:
        logging.warning(f"–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä PJAX –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∏–π: {str(e)}")
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–º–∏–ª–∫–∏
        with gzip.open(f"page_{page}_error.html.gz", "wt", encoding="utf-8") as f:
            f.write(driver.page_source)
        logging.info(f"HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page} –∑–±–µ—Ä–µ–∂–µ–Ω–æ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É –≤ page_{page}_error.html.gz")

    # –Ø–≤–Ω–µ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –≤–∞–∫–∞–Ω—Å—ñ–π
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "job-link"))
        )
    except Exception as e:
        logging.warning(f"–í–∞–∫–∞–Ω—Å—ñ—ó –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ: {str(e)}")
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–º–∏–ª–∫–∏
        with gzip.open(f"page_{page}_error.html.gz", "wt", encoding="utf-8") as f:
            f.write(driver.page_source)
        logging.info(f"HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page} –∑–±–µ—Ä–µ–∂–µ–Ω–æ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É –≤ page_{page}_error.html.gz")
        break

    # –Ü–º—ñ—Ç—É—î–º–æ –ø—Ä–æ–∫—Ä—É—á—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(random.uniform(1, 3))

    soup = BeautifulSoup(driver.page_source, "html.parser")
    job_listing = soup.find_all("div", class_="job-link")
    if not job_listing:
        print(f"‚ùå –í–∞–∫–∞–Ω—Å—ñ–π –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –∑—É–ø–∏–Ω—è—î–º–æ—Å—å.")
        logging.info(f"–í–∞–∫–∞–Ω—Å—ñ–π –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–º–∏–ª–∫–∏
        with gzip.open(f"page_{page}_error.html.gz", "wt", encoding="utf-8") as f:
            f.write(soup.prettify())
        logging.info(f"HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page} –∑–±–µ—Ä–µ–∂–µ–Ω–æ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É –≤ page_{page}_error.html.gz")
        no_results = soup.find(string=re.compile("–ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤|–í–∞–∫–∞–Ω—Å—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ"))
        if no_results:
            logging.info(f"–°—Ç–æ—Ä—ñ–Ω–∫–∞ {page} –º—ñ—Å—Ç–∏—Ç—å –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: {no_results}")
        break
    else:
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(job_listing)} –≤–∞–∫–∞–Ω—Å—ñ–π –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page}")
        if DEBUG_MODE:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –ª–∏—à–µ –∑–∞ —É–º–æ–≤–∏ DEBUG_MODE
            with gzip.open(f"page_{page}.html.gz", "wt", encoding="utf-8") as f:
                f.write(soup.prettify())
            logging.info(f"HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page} –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ page_{page}.html.gz")
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ —Ñ–∞–π–ª–∏ (—Å—Ç–∞—Ä—à–µ 5 —Å—Ç–æ—Ä—ñ–Ω–æ–∫)
            old_page = page - 5
            if old_page > 0 and os.path.exists(f"page_{old_page}.html.gz"):
                os.remove(f"page_{old_page}.html.gz")
                logging.info(f"–í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä–∏–π —Ñ–∞–π–ª page_{old_page}.html.gz")

    for job in job_listing:
        try:
            title_tag = job.find("h2")
            title = title_tag.text.strip() if title_tag else "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
            logging.info(f"–û–±—Ä–æ–±–∫–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó: {title}")

            company = "–ù–µ–≤—ñ–¥–æ–º–æ"
            company_div = job.find("div", class_="mt-xs")
            if company_div:
                company_tag = company_div.find("span", class_="strong-600")
                if company_tag:
                    company = company_tag.text.strip()
                    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–∞–Ω—ñ—é: {company}")
                else:
                    logging.warning(f"–¢–µ–≥ –∫–æ–º–ø–∞–Ω—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ div.mt-xs –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}")

            salary = "–ù–µ –≤–∫–∞–∑–∞–Ω–æ"
            salary_tag = job.find("span", class_="strong-600", string=re.compile(r"\d+[‚ÄØ‚Äâ]?\‚Äì[‚ÄØ‚Äâ]?\d+|\d+"))
            if salary_tag:
                salary = salary_tag.text.strip().replace("‚ÄØ", "").replace("‚Äâ", "").replace("–≥—Ä–Ω", "").replace(" ", "")
                logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞—Ä–ø–ª–∞—Ç—É: {salary}")
            else:
                logging.warning(f"–ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}")

            # –°–ø—Ä–æ–±–∞ –∑–Ω–∞–π—Ç–∏ –º—ñ—Å—Ç–æ –∫—ñ–ª—å–∫–æ–º–∞ –º–µ—Ç–æ–¥–∞–º–∏ —á–µ—Ä–µ–∑ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç—É
            # –ú–µ—Ç–æ–¥ 1: –ü–æ—à—É–∫ <span> –±–µ–∑ –∫–ª–∞—Å—É, —è–∫–∏–π –º–æ–∂–µ –º—ñ—Å—Ç–∏—Ç–∏ –º—ñ—Å—Ç–æ
            city = "–ù–µ –≤–∫–∞–∑–∞–Ω–æ"
            city_span = job.find("span", class_="")
            if city_span:
                city = city_span.text.strip().rstrip(",").strip()
                logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –º—ñ—Å—Ç–æ (–º–µ—Ç–æ–¥ 1): {city}")
            # –ú–µ—Ç–æ–¥ 2: –ü–æ—à—É–∫ <span> —ñ–∑ –∫–ª–∞—Å–æ–º "location"
            if city == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                city_tag_alt = job.find("span", class_="location")
                if city_tag_alt:
                    city = city_tag_alt.text.strip()
                    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –º—ñ—Å—Ç–æ (–º–µ—Ç–æ–¥ 2): {city}")
            # –ú–µ—Ç–æ–¥ 3: –ê–Ω–∞–ª—ñ–∑ <div class="mt-xs"> –¥–ª—è –ø–æ—à—É–∫—É —Ç–µ–∫—Å—Ç—É, —Å—Ö–æ–∂–æ–≥–æ –Ω–∞ –º—ñ—Å—Ç–æ
            if city == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                city_block = job.find("div", class_="mt-xs")
                if city_block:
                    found_company = False
                    for element in city_block.find_all(["span", "p"]):
                        text = element.text.strip()
                        if not found_company and element.find_parent("span", class_="mr-xs"):
                            found_company = True
                            continue
                        match = re.match(r"^[–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë\s,-]+", text)
                        if match:
                            city_text = match.group(0).rstrip(",").strip()
                            if not re.search(r"[()‚Ññ\d]", city_text) and not text.startswith(company.split()[0]):
                                city = city_text.split(",")[0].strip()
                                logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –º—ñ—Å—Ç–æ (–º–µ—Ç–æ–¥ 3): {city}")
                                break
            # –ú–µ—Ç–æ–¥ 4: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø–æ—à—É–∫—É –º—ñ—Å—Ç–∞
            if city == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                try:
                    city_span_new = job.select_one("div.mt-xs span:nth-child(3)")
                    if city_span_new:
                        city_text = city_span_new.text.strip().rstrip(",").strip()
                        if re.match(r"^[–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë\s,-]+$", city_text):
                            city = city_text.split(",")[0].strip()
                            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –º—ñ—Å—Ç–æ (–º–µ—Ç–æ–¥ 4): {city}")
                except Exception as e:
                    logging.warning(f"–ú–µ—Ç–æ–¥ 4 –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤ –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}: {str(e)}")
            if city == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                logging.warning(f"–ú—ñ—Å—Ç–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}. HTML –±–ª–æ–∫—É: {job.prettify()}")

            published_time = "–ù–µ –≤–∫–∞–∑–∞–Ω–æ"
            title_link = job.find("h2").find("a") if job.find("h2") else None
            if title_link and "title" in title_link.attrs:
                title_text = title_link["title"]
                match = re.search(r"–≤–∞–∫–∞–Ω—Å—ñ—è –≤—ñ–¥ (\d{1,2} [–∞-—è]+ \d{4})", title_text)
                if match:
                    published_time = match.group(1)
                    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –∑ –∞—Ç—Ä–∏–±—É—Ç—É title –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}: {published_time}")
            if published_time == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                time_tag = job.find("time")
                if time_tag:
                    if "datetime" in time_tag.attrs:
                        published_time = convert_iso_to_text(time_tag["datetime"])
                        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}: {published_time}")
                    else:
                        published_time = time_tag.text.strip()
                        logging.info(
                            f"–ó–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó (—Ç–µ–∫—Å—Ç–æ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç) –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}: {published_time}")
            if published_time == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
                logging.warning(f"–ß–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –≤–∞–∫–∞–Ω—Å—ñ—ó {title}. HTML –±–ª–æ–∫—É: {job.prettify()}")

            link_tag = job.find("h2").find("a") if job.find("h2") else None
            link = "https://www.work.ua" + link_tag["href"] if link_tag else "–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"

            normalized_title = unicodedata.normalize("NFKD", title.lower()).replace("—ñ", "i").replace("—ó", "i")
            vacancy_pattern = create_vacancy_pattern(search_vacancy)
            if re.search(vacancy_pattern, normalized_title, re.IGNORECASE):
                logging.info(f"–í–∞–∫–∞–Ω—Å—ñ—è {title} –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —à–∞–±–ª–æ–Ω—É")
                if city == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ" or search_city in city.lower():
                    jobs.append([title, company, salary, city, published_time, link])
                    print(
                        f"–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó: {title} | –ö–æ–º–ø–∞–Ω—ñ—è: {company} | –ó–∞—Ä–ø–ª–∞—Ç–∞: {salary} | –ú—ñ—Å—Ç–æ: {city} | –ß–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó: {published_time}")
                else:
                    logging.info(
                        f"–í–∞–∫–∞–Ω—Å—ñ—è {title} –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ –Ω–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –º—ñ—Å—Ç–∞: {city} (–æ—á—ñ–∫—É—î—Ç—å—Å—è {search_city})")
            else:
                logging.warning(f"–ù–∞–∑–≤–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó {title} –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —à–∞–±–ª–æ–Ω—É")

        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –≤–∞–∫–∞–Ω—Å—ñ—ó {title}: {str(e)}")

    print(f"‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ {page} –æ–±—Ä–æ–±–ª–µ–Ω–∞!")
    logging.info(f"–°—Ç–æ—Ä—ñ–Ω–∫–∞ {page} –æ–±—Ä–æ–±–ª–µ–Ω–∞")
    page += 1
    time.sleep(random.uniform(1, 3))

# –ó–∞–∫—Ä–∏–≤–∞—î–º–æ –±—Ä–∞—É–∑–µ—Ä
driver.quit()


def parse_salary(salary):
    """
    –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î —Ç–µ–∫—Å—Ç–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∏ —É —á–∏—Å–ª–æ–≤–µ –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è.

    Args:
        salary (str): –ó–∞—Ä–ø–ª–∞—Ç–∞ —É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "20000", "30000‚Äì40000" –∞–±–æ "–ù–µ –≤–∫–∞–∑–∞–Ω–æ").

    Returns:
        float: –°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∏ –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è (–∞–±–æ 0, —è–∫—â–æ –∑–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ –≤–∫–∞–∑–∞–Ω–∞).
    """
    if salary == "–ù–µ –≤–∫–∞–∑–∞–Ω–æ":
        return 0
    try:
        if "‚Äì" in salary:
            low, high = map(int, salary.split("‚Äì"))
            return (low + high) / 2
        return int(salary)
    except ValueError:
        return 0


jobs.sort(key=lambda x: parse_salary(x[2]), reverse=True)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É CSV
if jobs:
    with open("workua_jobs.csv", "w", newline="", encoding="UTF-8") as file:
        writer = csv.writer(file)
        writer.writerow(["–ù–∞–∑–≤–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó", "–ö–æ–º–ø–∞–Ω—ñ—è", "–ó–∞—Ä–ø–ª–∞—Ç–∞", "–ú—ñ—Å—Ç–æ", "–ß–∞—Å –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó", "–ü–æ—Å–∏–ª–∞–Ω–Ω—è"])
        writer.writerows(jobs)
    print("‚úÖ –í–∞–∫–∞–Ω—Å—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ workua_jobs.csv")
    logging.info("–í–∞–∫–∞–Ω—Å—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ workua_jobs.csv")
else:
    print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó –≤–∞–∫–∞–Ω—Å—ñ—ó –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.")
    logging.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó –≤–∞–∫–∞–Ω—Å—ñ—ó")
